{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_sketch_rnn_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-FREWjP1Wbz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './gdrive/MyDrive/Pytorch_SketchRNN/Data/'\n",
        "batch_size = 100\n",
        "dim_z = 128\n",
        "input_size = 5 # vectorized data: (del x, del y, p1, p2, p3)\n",
        "num_mix_components = 20\n",
        "dropout = 0.9\n",
        "eta_min = 0.01\n",
        "temperature = 0.4\n",
        "encoder_hidden_size = 256\n",
        "decoder_hidden_size = 512\n",
        "max_seq_length = 200\n",
        "grad_clip = 1.\n",
        "longest_seq_len = 0\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "kfV2YqBA7fkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StrokesDataset(Dataset):\n",
        "  def __init__(self, dataset, max_seq_length, scale = None):\n",
        "    global longest_seq_len\n",
        "    \n",
        "    data = []\n",
        "\n",
        "    for seq in dataset:\n",
        "      if 10 < len(seq) <= max_seq_length:\n",
        "        seq = np.minimum(seq, 1000)\n",
        "        seq = np.maximum(seq, -1000)\n",
        "\n",
        "        seq = np.array(seq, dtype=np.float32)\n",
        "        data.append(seq)\n",
        "    \n",
        "    if scale is None:\n",
        "      scale = np.std(np.concatenate([np.ravel(s[:, 0:2]) for s in data]))\n",
        "    self.scale = scale\n",
        "\n",
        "    longest_seq_len = max([len(seq) for seq in data])\n",
        "    self.data = torch.zeros(len(data), longest_seq_len + 2, 5, dtype=torch.float)\n",
        "    self.mask = torch.zeros(len(data), longest_seq_len + 1)\n",
        "    \n",
        "    for i, seq in enumerate(data):\n",
        "      seq = torch.from_numpy(seq)\n",
        "      len_seq = len(seq)\n",
        "\n",
        "      self.data[i, 1:len_seq + 1, :2] = seq[:, :2] / scale\n",
        "      \n",
        "      self.data[i, 1:len_seq + 1, 2] = 1 - seq[:, 2]\n",
        "      self.data[i, 1:len_seq + 1, 3] = seq[:, 2]\n",
        "      self.data[i, len_seq + 1:, 4] = 1\n",
        "      self.mask[i, :len_seq + 1] = 1\n",
        "    \n",
        "    self.data[:, 0, 2] = 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.mask[idx]"
      ],
      "metadata": {
        "id": "K3xYaMlwHxq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_size(data):\n",
        "    \"\"\"larger sequence length in the data set\"\"\"\n",
        "    sizes = [len(seq) for seq in data]\n",
        "    return max(sizes)"
      ],
      "metadata": {
        "id": "GiV3Q0OhWKq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def purify(strokes):\n",
        "    \"\"\"removes to small or too long sequences + removes large gaps\"\"\"\n",
        "    data = []\n",
        "    for seq in strokes:\n",
        "        if seq.shape[0] <= max_seq_length and seq.shape[0] > 10:\n",
        "            seq = np.minimum(seq, 1000)\n",
        "            seq = np.maximum(seq, -1000)\n",
        "            seq = np.array(seq, dtype=np.float32)\n",
        "            data.append(seq)\n",
        "    return data"
      ],
      "metadata": {
        "id": "NKavzVccWNP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_normalizing_scale_factor(strokes):\n",
        "    \"\"\"Calculate the normalizing factor explained in appendix of sketch-rnn.\"\"\"\n",
        "    data = []\n",
        "    for i in range(len(strokes)):\n",
        "        for j in range(len(strokes[i])):\n",
        "            data.append(strokes[i][j, 0])\n",
        "            data.append(strokes[i][j, 1])\n",
        "    data = np.array(data)\n",
        "    return np.std(data)"
      ],
      "metadata": {
        "id": "poVm41l6WPds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(strokes):\n",
        "    \"\"\"Normalize entire dataset (delta_x, delta_y) by the scaling factor.\"\"\"\n",
        "    data = []\n",
        "    scale_factor = calculate_normalizing_scale_factor(strokes)\n",
        "    for seq in strokes:\n",
        "        seq[:, 0:2] /= scale_factor\n",
        "        data.append(seq)\n",
        "    return data"
      ],
      "metadata": {
        "id": "1iJhn49lWRbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.load(data_path+'cat.npz', encoding='latin1', allow_pickle=True)\n",
        "data = dataset['train']\n",
        "data = purify(data)\n",
        "data = normalize(data)\n",
        "Nmax = max_size(data)"
      ],
      "metadata": {
        "id": "c7AntqjTWSxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(batch_size):\n",
        "    # batch_idx = np.random.choice(len(data),batch_size)\n",
        "    batch_idx = np.arange(0,101)\n",
        "    batch_sequences = [data[idx] for idx in batch_idx]\n",
        "    strokes = []\n",
        "    lengths = []\n",
        "    indice = 0\n",
        "    for seq in batch_sequences:\n",
        "        len_seq = len(seq[:,0])\n",
        "        new_seq = np.zeros((Nmax,5))\n",
        "        new_seq[:len_seq,:2] = seq[:,:2]\n",
        "        new_seq[:len_seq-1,2] = 1-seq[:-1,2]\n",
        "        new_seq[:len_seq,3] = seq[:,2]\n",
        "        new_seq[(len_seq-1):,4] = 1\n",
        "        new_seq[len_seq-1,2:4] = 0\n",
        "        lengths.append(len(seq[:,0]))\n",
        "        strokes.append(new_seq)\n",
        "        indice += 1\n",
        "        # if indice < 2: print(new_seq)\n",
        "    batch = Variable(torch.from_numpy(np.stack(strokes,1)).to(device).float())\n",
        "    return batch, lengths"
      ],
      "metadata": {
        "id": "iWspvaclWnBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BivariateGaussianMixture:\n",
        "  def __init__(self, pi_cat_probs, mu_x, mu_y, sig_x, sig_y, rho_xy):\n",
        "    self.pi_cat_probs = pi_cat_probs\n",
        "    self.mu_x = mu_x\n",
        "    self.mu_y = mu_y\n",
        "    self.sig_x = sig_x\n",
        "    self.sig_y = sig_y\n",
        "    self.rho_xy = rho_xy\n",
        "\n",
        "  def set_temperature(self, temperature):\n",
        "    self.pi_cat_probs /= temperature\n",
        "    self.sig_x *= math.sqrt(temperature)\n",
        "    self.sig_y *= math.sqrt(temperature)\n",
        "\n",
        "  def get_distribution(self):\n",
        "    \n",
        "    sig_x = torch.clamp_min(self.sig_x, 1e-5)\n",
        "    sig_y = torch.clamp_min(self.sig_y, 1e-5)\n",
        "    rho_xy = torch.clamp(self.rho_xy, 1e-5 - 1, 1 - 1e-5)\n",
        "\n",
        "    mean = torch.stack([self.mu_x, self.mu_y], -1)\n",
        "\n",
        "    cov = torch.stack([\n",
        "            sig_x * sig_x, rho_xy * sig_x * sig_y,\n",
        "            rho_xy * sig_x * sig_y, sig_y * sig_y\n",
        "          ], -1)\n",
        "    cov = cov.view(*sig_y.shape, 2, 2)\n",
        "\n",
        "    bi_dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=cov)\n",
        "    cat_dist = torch.distributions.Categorical(logits=self.pi_cat_probs)\n",
        "\n",
        "    return bi_dist, cat_dist\n",
        "\n",
        "  def bivariate_normal_pdf(self, dx, dy):\n",
        "    z_x = ((dx-self.mu_x)/self.sig_x)**2\n",
        "    z_y = ((dy-self.mu_y)/self.sig_y)**2\n",
        "    z_xy = (dx-self.mu_x)*(dy-self.mu_y)/(self.sig_x*self.sig_y)\n",
        "    z = z_x + z_y -2*self.rho_xy*z_xy\n",
        "    exp = torch.exp(-z/(2*(1-self.rho_xy**2)))\n",
        "    norm = 2*np.pi*self.sig_x*self.sig_y*torch.sqrt(1-self.rho_xy**2)\n",
        "    return exp/norm"
      ],
      "metadata": {
        "id": "-u7z_LAJ4I1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "\n",
        "    self.rnn_lstm = nn.LSTM(input_size, encoder_hidden_size, dropout=dropout, bidirectional=True)\n",
        "    self.mean_linear = nn.Linear(2 * encoder_hidden_size, dim_z)\n",
        "    self.sigma_linear = nn.Linear(2 * encoder_hidden_size, dim_z)\n",
        "    # self.train()\n",
        "    \n",
        "  def forward(self, input, batch_size, state=None):\n",
        "    if state is None:\n",
        "       state = (torch.zeros(2, batch_size, encoder_hidden_size).to(device), torch.zeros(2, batch_size, encoder_hidden_size).to(device))\n",
        "    _, (hn, cn) = self.rnn_lstm(input.float(), state)\n",
        "\n",
        "    hidden_forward, hidden_backward = torch.split(hn,1,0)\n",
        "    hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
        "    mean = self.mean_linear(hidden_cat)\n",
        "    sigma = self.sigma_linear(hidden_cat)\n",
        "    std = torch.exp(sigma / 2.)\n",
        "    # noisy latent vector\n",
        "    z = mean + sigma * torch.normal(mean.new_zeros(mean.shape), mean.new_ones(mean.shape)).to(device)\n",
        "    # Adding random noises to standard deviation prevents deterministic results \n",
        "\n",
        "    return z, mean, sigma"
      ],
      "metadata": {
        "id": "vA1dEFe15KZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(DecoderRNN, self).__init__()\n",
        "\n",
        "      self.init_state = nn.Linear(dim_z, 2 * decoder_hidden_size)\n",
        "      self.rnn_lstm = nn.LSTM(dim_z + 5, decoder_hidden_size)\n",
        "\n",
        "      self.fc_layer = nn.Linear(decoder_hidden_size, 6 * num_mix_components + 3) # num_mix*(weight, mu_x, mu_y, sig_x, sig_y, ro_xy) + 3 states\n",
        "\n",
        "    def forward(self, input, z, state=None):\n",
        "\n",
        "      if state is None:\n",
        "        h_0, c_0 = torch.split(torch.tanh(self.init_state(z)), decoder_hidden_size, 1)\n",
        "        state = (h_0.unsqueeze(0).contiguous(), c_0.unsqueeze(0).contiguous())\n",
        "\n",
        "      outputs, (h_0,c_0) = self.rnn_lstm(input, state)\n",
        "\n",
        "      if self.training:\n",
        "        outputs = self.fc_layer(outputs.view(-1, decoder_hidden_size))\n",
        "        out_len = longest_seq_len + 1\n",
        "      else:\n",
        "        outputs = self.fc_layer(h_0.view(-1, decoder_hidden_size))\n",
        "        out_len = 1\n",
        "\n",
        "      separated_outputs = torch.split(outputs,6,1)\n",
        "      mixture_outputs = torch.stack(separated_outputs[:-1])\n",
        "      q_outputs = separated_outputs[-1]\n",
        "      \n",
        "      pi_cat_probs, mu_x, mu_y, sig_x, sig_y, rho_xy = torch.split(mixture_outputs, 1, 2)      \n",
        "\n",
        "      pi_cat_probs = F.softmax(pi_cat_probs.transpose(0,1).squeeze()).view(out_len,-1,num_mix_components)\n",
        "      sig_x = torch.exp(sig_x.transpose(0,1).squeeze()).view(out_len,-1,num_mix_components)\n",
        "      sig_y = torch.exp(sig_y.transpose(0,1).squeeze()).view(out_len,-1,num_mix_components)\n",
        "      rho_xy = torch.tanh(rho_xy.transpose(0,1).squeeze()).view(out_len,-1,num_mix_components)\n",
        "      mu_x = mu_x.transpose(0,1).squeeze().contiguous().view(out_len,-1,num_mix_components)\n",
        "      mu_y = mu_y.transpose(0,1).squeeze().contiguous().view(out_len,-1,num_mix_components)\n",
        "      q_cat_probs = F.softmax(q_outputs).view(out_len,-1,3)\n",
        "\n",
        "      dist = BivariateGaussianMixture(pi_cat_probs,mu_x,mu_y,sig_x,sig_y,rho_xy)\n",
        "\n",
        "      return dist, q_cat_probs, h_0, c_0"
      ],
      "metadata": {
        "id": "ZlBG5C_uLMjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReconstructionLoss(nn.Module):\n",
        "  def forward(self, target, mask, dist, q_logits):\n",
        "    bi_dist, cat_dist = dist.get_distribution()\n",
        "    xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, num_mix_components, -1)\n",
        "    # dx = torch.stack([target[:,:,0]]*num_mix_components,2)\n",
        "    # dy = torch.stack([target[:,:,1]]*num_mix_components,2)\n",
        "    probs = torch.sum(cat_dist.probs * torch.exp(bi_dist.log_prob(xy)), 2)\n",
        "    loss_stroke = -torch.mean(mask * torch.log(1e-5 + probs))\n",
        "    \n",
        "    loss_pen = -torch.mean(target[:, :, 2:] * q_logits)\n",
        "    return loss_stroke + loss_pen"
      ],
      "metadata": {
        "id": "YgrX1yEBMSZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KLDivLoss(nn.Module):\n",
        "  def forward(self, sigma, mean):\n",
        "    return -0.5 * torch.mean(1 + sigma - mean ** 2 - torch.exp(sigma))"
      ],
      "metadata": {
        "id": "w-3nNZokO3_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampler:\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  def sample_conditional(self, data, temperature):\n",
        "    self.model.eval().to(device)\n",
        "    z, _, _ = self.model.encoder(data,1)\n",
        "    return self.sample(z, temperature)\n",
        "\n",
        "  def sample_unconditional(self, temperature):\n",
        "    self.model.eval().to(device)\n",
        "    z = torch.randn(1, dim_z, dtype=torch.float)\n",
        "    return self.sample(z, temperature)\n",
        "\n",
        "  def sample(self, z, temperature):\n",
        "    tensor = torch.ones(5, dtype=torch.float32).to(device)\n",
        "    s = tensor.new_tensor([0, 0, 1, 0, 0])\n",
        "    seq = [s]\n",
        "    state = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i in range(max_seq_length):\n",
        "        data = torch.cat([s.view(1, 1, -1), z.unsqueeze(0)], 2)\n",
        "        dist, q_cat_probs, h0, c0 = self.model.decoder(data, z, state)\n",
        "        s = self._sample_step(dist, q_cat_probs, temperature)\n",
        "        seq.append(s)\n",
        "        if s[4] == 1:\n",
        "          break\n",
        "    \n",
        "    seq = torch.stack(seq)\n",
        "    self.plot(seq)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _sample_step(dist, q_cat_probs, temperature):\n",
        "    dist.set_temperature(temperature)\n",
        "    bi_dist, cat_dist = dist.get_distribution()\n",
        "    idx = cat_dist.sample()[0, 0]\n",
        "    q = torch.distributions.Categorical(logits=q_cat_probs / temperature)\n",
        "    q_idx = q.sample()[0, 0]\n",
        "    xy = bi_dist.sample()[0, 0, idx]\n",
        "    stroke = q_cat_probs.new_zeros(5)\n",
        "    stroke[:2] = xy\n",
        "    stroke[q_idx + 2] = 1\n",
        "    return stroke\n",
        "\n",
        "  @staticmethod\n",
        "  def plot(seq: torch.Tensor):\n",
        "    seq[:, 0:2] = torch.cumsum(seq[:, 0:2], dim=0)\n",
        "    seq[:, 2] = seq[:, 3]\n",
        "    seq = seq[:, 0:3].detach().cpu().numpy()\n",
        "    strokes = np.split(seq, np.where(seq[:, 2] > 0)[0] + 1)\n",
        "    for s in strokes:\n",
        "      plt.plot(s[:, 0], -s[:, 1])\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bXj2VxLoX2EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SketchRNN_Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = EncoderRNN().to(device)\n",
        "    self.decoder = DecoderRNN().to(device)\n",
        "\n",
        "    self.loss_kl = KLDivLoss().to(device);\n",
        "    self.loss_rec = ReconstructionLoss().to(device);\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    z, mean, sigma = self.encoder(inputs, batch_size)\n",
        "\n",
        "    z_stack = torch.stack([z]*(longest_seq_len+1))\n",
        "    inputs = torch.cat([inputs[:-1], z_stack], 2)\n",
        "\n",
        "    dist, q_cat_probs, _, _ = self.decoder(inputs, z, None)\n",
        "\n",
        "    return dist, q_cat_probs, mean, sigma"
      ],
      "metadata": {
        "id": "ee6NWbMA7rpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(model, batch):\n",
        "  model.train()\n",
        "\n",
        "  data = batch[0].to(device).transpose(0, 1)\n",
        "  mask = batch[1].to(device).transpose(0, 1)\n",
        "  \n",
        "  # model forward\n",
        "  dist, q_cat_probs, mean, sigma = model(data)\n",
        "\n",
        "  # compute losses\n",
        "  loss_kl = model.loss_kl(mean, sigma)\n",
        "  loss_draw = model.loss_rec(data[1:], mask, dist, q_cat_probs)\n",
        "  loss = loss_kl + loss_draw\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "5pF-srkbEq7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, epoch, batch, valid_dataset, optimizer, sampler):\n",
        "  loss = step(model, batch)\n",
        "  loss.backward()\n",
        "  nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "  optimizer.step()\n",
        "  \n",
        "  if epoch%1==0:\n",
        "    print('epoch',epoch,'loss',loss.item())\n",
        "  if epoch%100==0:\n",
        "    data, *_ = valid_dataset[np.random.choice(len(valid_dataset))]\n",
        "    data = data.unsqueeze(1).to(device)\n",
        "    sampler.sample_conditional(data,1)"
      ],
      "metadata": {
        "id": "bUfzp4LOKTd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.load(data_path+'cat.npz', encoding='latin1', allow_pickle=True)\n",
        "\n",
        "train_data = StrokesDataset(dataset['train'], 200)\n",
        "valid_data = StrokesDataset(dataset['valid'], 200)\n",
        "\n",
        "# rand_sampler = torch.utils.data.RandomSampler(train_data, num_samples=1, replacement=True)\n",
        "train_loader = DataLoader(train_data, batch_size)\n",
        "valid_loader = DataLoader(valid_data, batch_size)"
      ],
      "metadata": {
        "id": "2ZWArLmFFaly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SketchRNN_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "sampler = Sampler(model)"
      ],
      "metadata": {
        "id": "b33bVzc0TrVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch, batch in enumerate(train_loader):\n",
        "   train_epoch(model, epoch, batch, valid_data, optimizer, sampler)"
      ],
      "metadata": {
        "id": "xSpsVkiJUKqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch, batch in enumerate(train_loader):\n",
        "  if(epoch==0):\n",
        "     print(batch)\n",
        "     break"
      ],
      "metadata": {
        "id": "W8A3t588TIyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(seq: torch.Tensor):\n",
        "  seq[:, 0:2] = torch.cumsum(seq[:, 0:2], dim=0)\n",
        "  seq[:, 2] = seq[:, 3]\n",
        "  seq = seq[:, 0:3].detach().cpu().numpy()\n",
        "  strokes = np.split(seq, np.where(seq[:, 2] > 0)[0] + 1)\n",
        "  for s in strokes:\n",
        "    plt.plot(s[:, 0], -s[:, 1])\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Mn28uk1NXJgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0][0]"
      ],
      "metadata": {
        "id": "t_0-oPI7Y2CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jIHehczQY64a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}